{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FARE and E-FARE Models Example with the Adult dataset\n",
    "\n",
    "In this notebook we will show how to use the FARE method to generate counterfactual interventions for the Adult dataset ([Dua and Graff (2019)](https://archive.ics.uci.edu/ml/citation_policy.html)). We will show how to train and perform inference with FARE, by looking at a black-box model. In this tutorial, we use a support vector machine (SVM) with an RBF kernel as the black-box model. In the paper, we used instead a trained MLP, which offers a more challenging scenario. You can find the original paper below:\n",
    "\n",
    "[De Toni, G., Lepri, B. & Passerini, A. Synthesizing explainable counterfactual policies for algorithmic recourse with program synthesis. Mach Learn (2023)](https://link.springer.com/article/10.1007/s10994-022-06293-7)\n",
    "\n",
    "**If have any questions or if you spot any issue with the following notebook, you can reach me at [giovanni.detoni@unitn.it](giovanni.detoni@unitn.it)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import the SVC class and some additional preprocessing methods\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import make_column_selector, make_column_transformer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# We just need to import the FARE model from rl_mcts\n",
    "from recourse_fare.models.FARE import FARE\n",
    "from recourse_fare.models.EFARE import EFARE\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import torch\n",
    "\n",
    "# It is done to make the notebook aware of additional classes.\n",
    "# It is the equivalent of setting the PYTHONPATH in the local env.\n",
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some random seeds to ensure reproducibility\n",
    "random.seed(2023)\n",
    "np.random.seed(2023)\n",
    "torch.manual_seed(2023)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "We first retrieve the Adult dataset and we perform some data cleaning activity. We first split the dataset into training and test, then we mainly perform two actions:\n",
    "* We convert the `income_target` variable to either 0 (`>50K`) or 1 (`<=50K`);\n",
    "* We replace entries with `?` with the most frequent element for that given column. The most frequent element is taken by looking at the training set only and then by using those values to input the test set;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read adult dataset and preprocess them.\n",
    "X = pd.read_csv(\"../data/adult_score/train.csv\")\n",
    "\n",
    "# Drop NaNs in the dataset\n",
    "X.dropna(inplace=True)\n",
    "\n",
    "# We some columns we do not consider actionable. It makes the problem less interesting, but it does\n",
    "# show the point about how counterfactual interventions works. \n",
    "X.drop(columns=[\"fnlwgt\", \"age\", \"education_num\", \"race\", \"sex\", \"native_country\", \"relationship\"], inplace=True)\n",
    "\n",
    "y = X.income_target.apply(lambda x: 1 if x==\"<=50K\" else 0)\n",
    "X.drop(columns=[\"income_target\", \"predicted\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train/test by first removing those entries with NaNs\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the most frequent attributes for the features with '?' values.\n",
    "# We compute the most frequent attributes by looking at the training set only.\n",
    "attrib, counts = np.unique(X_train['workclass'], return_counts = True)\n",
    "most_freq_attrib_w = attrib[np.argmax(counts, axis = 0)]\n",
    "\n",
    "attrib, counts = np.unique(X_train['occupation'], return_counts = True)\n",
    "most_freq_attrib_o = attrib[np.argmax(counts, axis = 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a simple utility function which replaces the '?' with custom values\n",
    "def clean(data, most_freq_attrib_o, most_freq_attrib_w, most_freq_attrib_c):\n",
    "    data['occupation'] = data['occupation'].apply(lambda x: most_freq_attrib_o if x=='?' else x)\n",
    "    data['workclass'] = data['workclass'].apply(lambda x: most_freq_attrib_w if x=='?' else x)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We apply the clean() function both to the training and test set.\n",
    "X_train = clean(X_train, most_freq_attrib_o, most_freq_attrib_w, None)\n",
    "X_test = clean(X_test, most_freq_attrib_o, most_freq_attrib_w, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Pipeline with scikit-learn\n",
    "\n",
    "We then build a preprocessor to standardize/encode the various features. Namely, we use a `StandardScaler` and an `OneHotEncoder` to manage the real and categorical features, respectively. We exploit the `ColumnTransformer` class of scikit-learn to build a complete preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a preprocessing pipeline, which can be used to preprocess\n",
    "# the elements of the dataset.\n",
    "cat_selector = make_column_selector(dtype_include=object)\n",
    "num_selector = make_column_selector(dtype_include=np.number)\n",
    "preprocessor = make_column_transformer(\n",
    "    (StandardScaler(), num_selector), (OneHotEncoder(handle_unknown=\"ignore\",sparse=False), cat_selector)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the preprocessor on the training data \n",
    "preprocessor.fit(X_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluating the black-box model\n",
    "\n",
    "Given the preprocessor, we train a simple `SVC` class in a \"balanced\" mode, where the misclassification errors are weighted by relative numerosity of a class. Adult is an unbalanced dataset, therefore we need to make sure that our model has a decent F1 score, rather than just the accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.55      0.67       497\n",
      "           1       0.77      0.94      0.85       806\n",
      "\n",
      "    accuracy                           0.79      1303\n",
      "   macro avg       0.81      0.75      0.76      1303\n",
      "weighted avg       0.80      0.79      0.78      1303\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit a simple model over the data\n",
    "blackbox_model = SVC(class_weight=\"balanced\")\n",
    "blackbox_model.fit(preprocessor.transform(X_train), y_train)\n",
    "\n",
    "# Evaluate the model and print the classification report for the two classes\n",
    "output = blackbox_model.predict(preprocessor.transform(X_test))\n",
    "print(classification_report(output, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the training dataset by picking only the examples which are classified negatively by the model\n",
    "output = blackbox_model.predict(preprocessor.transform(X_train))\n",
    "X_train[\"predicted\"] = output\n",
    "X_train = X_train[X_train.predicted == 1]\n",
    "X_train.drop(columns=\"predicted\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FARE Model\n",
    "\n",
    "The FARE model needs three different cofigurations parameters:\n",
    "* **Policy Configuration**: it specifies how the internal agent will be built. Please have a look at Figure 3 of the paper to understand the policy architecture.\n",
    "* **Environment Configuration**: it specifies the environment where our agent will work on. Please have a look at the original implementation `rl_mcts/example/mock_adult_env.py` to understand its internal components.\n",
    "* **MCTS Configuration**: it specifies some hyperparameters of the MCTS search component. \n",
    "\n",
    "In the cell below we show what we think are the most important configuration parameters for each component. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_config= {\n",
    "    \"observation_dim\": 46, # Size of the state (after using the preprocessor defined above)\n",
    "    \"encoding_dim\": 30, # Size of the output embedding of the state encoder \n",
    "    \"hidden_size\": 30 # Size of the hiddel layers of the controller (LSTM)\n",
    "}\n",
    "\n",
    "environment_config = {\n",
    "    \"class_name\": \"recourse_fare.example.mock_adult_env.AdultEnvironment\", # Class implementing the environment.\n",
    "    \"additional_parameters\": {\n",
    "        \"preprocessor\": preprocessor    # ColumnTransformer which is used to parse the environment. \n",
    "                                        # It is not a mandatory argument, but it is required by the AdultEnvironment class.\n",
    "    }\n",
    "}\n",
    "\n",
    "mcts_config = {\n",
    "    \"number_of_simulations\": 10, # How many simulations we want to perform at each MCTS node.\n",
    "    \"dir_epsilon\": 0.3, # Parameter trading off exploration and exploitation (1.0 = only exploration).\n",
    "    \"dir_noise\": 0.3 # Concentration parameter of the Dirichlet distribution used as \"noise\".\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a FARE model by giving to the constructor the `blackbox_model` and the various configurations defined above. The `batch_size` indicates how many succesful samples we need at each training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a FARE model given the previous configurations\n",
    "model = FARE(blackbox_model, policy_config, environment_config, mcts_config, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the FARE model\n",
    "\n",
    "If you have done all correctly, training the FARE model is as easy as calling the fit method as shown below. However, the training accuracy might struggle to increase to a satisfactory level for many different reasons. Here we give some empirical suggestions we learned during our experiments:\n",
    "1. **Increase the `max_depth_dict` value**. Such value indicates how long an intervention can be (e.g., how many actions the user might want to perform). Therefore, if a user needs to modify N of its features to obtain recourse, a `max_depth_dict` lower than `N` might make the model unable to learn succesful interventions. As a rule of thumb, we suggest to set the `max_depth_dict` value to at least $\\frac{N}{2}$, where $N$ is the number of features.\n",
    "2. **Improve the actions**. It might happen that the actions defined in the environment class might not be enough to obtain recourse. This situation might arise in two cases: either we are supplying to few arguments to an action (it means we might miss some potential good changes which could lead to recourse), or either the black-box classifier is making decisions based on non-actionable features (e.g., age, sex, native country etc.). In the latter case, there is little we can do, and it could be a hint that we trained an \"unfair\" model.\n",
    "3. **Increase the `dir_epsilon` value**. Sometimes, it might happen that we do not provide enough noise to explore a potentially large actiona space. Thus, limiting the overall exploration.\n",
    "\n",
    "If we specify the `tensorboard` argument, the FARE model will save training stats in a directory, where they can be readily read by using tensorboard. This feature is particularly useful to check if the training is continuing correctly or if we need to take steps based on the suggestions above. If you want to display the information through tensorboard, then you just need to run:\n",
    "```\n",
    "tensorboard --logdir ../notebooks/runs/adult\n",
    "```\n",
    "\n",
    "#### Note on the `max_iter` parameter\n",
    "\n",
    "The `max_iter` parameter had a double meaning. It corresponds to how many potentially **different** user we sample from the training dataset and how many **training steps** we are performing. So, if we set 5000 as `max_iter`, we are sure that we are going to sample at least 5000 users. However, we might perform a lower number of training steps (e.g., gradient updates). This behaviour depends on the `batch_size` parameter which indicates the minimum number of examples (sampled from the training buffer) we can use to train the model. It might happen that finding good interventions via MCTS is hard, thus the training buffer gets slowly populated over each iterations. Therefore, until we get `len(training_buffer) >= batch_size`, we will not start training the underlying agent.       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa89230bbfc4da8bb94554c27217d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train FARE:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We fit the FARE model with tensorboard enable.\n",
    "# We need to make sure that the directory `../notebooks/runs/adult` exists\n",
    "# since it is not created automatically\n",
    "model.fit(X_train, max_iter=500, tensorboard=\"../notebooks/runs/adult\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We save the trained FARE model to disk\n",
    "model.save(\"/tmp/fare_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a pre-trained FARE model for the Adult dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load a pretrained model from a previous checkpoint\n",
    "pretrained_model = \"../notebooks/models/fare_adult-17_03_2023.pth\"\n",
    "model.load(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the model to predict the test data\n",
    "# We use only the test data which are negatively classified\n",
    "output = blackbox_model.predict(preprocessor.transform(X_test))\n",
    "X_test[\"predicted\"] = output\n",
    "X_test = X_test[X_test.predicted == 1]\n",
    "X_test.drop(columns=\"predicted\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we load the pretrained FARE model, we can run inference over the test instances. By specifying `full_output=True`, we obtain additional information from the model. Namely, we obtain: the **counterfactual istances**, the **recourse results** (1 if we obtained recourse, 0 otherwise), the **counterfactual interventions** and the **costs** of those interventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "159f4561b61b4bda8702535192251714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval FARE:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We run inference using FARE over 10 examples taken from the test set\n",
    "counterfactuals, has_reached_recourse, traces, costs, _ = model.predict(X_test[0:100], full_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute the $validity$, fraction of succesfull interventions we can find:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Validity: {sum(has_reached_recourse)/(len(has_reached_recourse))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also extract an example of a counterfactual intervention (with both actions and arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "print(pprint.pformat(traces[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train an E-FARE deterministic model\n",
    "\n",
    "We now show how to train the deterministic model E-FARE, which will allow us to extract an automaton from the successful interventions. E-FARE produces also boolean rules which explains why the model suggested a certain action in the intervention. The interface of the `EFARE` class is similar to the `FARE` API. For the details of the procedure, we point the reader to Section 3.6 and Section 3.7 of the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing for E-FARE\n",
    "\n",
    "E-FARE trains a series of decision trees, one for each action available. If we want to have interpretable rules, then we need to make our observations interpretable. For the adult dataset, we keep the numerical values as they are, but we one hot encode the categorical values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_selector = make_column_selector(dtype_include=object)\n",
    "preprocessor_efare = make_column_transformer(\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\",sparse=False), cat_selector), remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "# We fit the EFARE preprocessor\n",
    "preprocessor_efare.fit(X_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The E-FARE model needs as arguments only the pretrained FARE model and a preprocessor (which could be the same used by the FARE model). Then, training it and saving it to disk are straightforward steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We instantiate the EFARE model, we train it over 100 examples from the training set and we save it to disk.\n",
    "efare_model = EFARE(model, preprocessor_efare)\n",
    "efare_model.fit(X_train[0:100], verbose=True)\n",
    "efare_model.save(\"/tmp/efare_adult.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we did for the FARE model, we load a pre-trained E-FARE model which will give us a decent validity\n",
    "efare_model.load(\"../notebooks/models/efare_adult-17_03_2023.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `predict` API is the same as the `FARE` model. The only difference is that it returns also the **rules** from the decision trees for each action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We run inference using E-FARE\n",
    "counterfactuals, has_reached_recourse, traces, costs, rules = efare_model.predict(X_test[0:100], full_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define now a small function to parse the rules in a intelligible shape. It might be a bit cumbersome and it depends on the preprocessing step and if it is used or no. However, it is just for markup purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean up the rules extracted from EFARE\n",
    "def clean_rules(rules):\n",
    "    new_rule = []\n",
    "    for single_rule in rules:\n",
    "        tmp_rule = []\n",
    "        for clause in single_rule:\n",
    "            if \"onehotencoder__\" in clause:\n",
    "                # This branch checks if the rule comes from an one-hot-encoded variable\n",
    "                clause = clause.replace(\"onehotencoder__\", \"\")             \n",
    "                if \"<= 0.5\" in clause:\n",
    "                    negation = \"not\"\n",
    "                else:\n",
    "                    negation = \"\"            \n",
    "                clause = clause.replace(\"<= 0.5\", \"\")\n",
    "                clause = clause.replace(\"> 0.5\", \"\")            \n",
    "                feature, value = clause.rsplit('_', 1)            \n",
    "                final_clause = negation+\" \"+feature+\" = \"+value\n",
    "                tmp_rule.append(final_clause.strip())             \n",
    "            elif \"remainder__\" in clause:\n",
    "                clause = clause.replace(\"remainder__\", \"\")    \n",
    "                tmp_rule.append(clause)     \n",
    "        new_rule.append(\" and \".join(tmp_rule))\n",
    "    return new_rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have this function, we can easily use it to compute the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can print the rules for a given user\n",
    "import pprint\n",
    "\n",
    "print(\"Example of rules extracted by EFARE:\")\n",
    "for action, rule in zip(traces[0], clean_rules(rules[0])):\n",
    "    print(action, \"\\t\", rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last, we compute the validity of EFARE. Clearly, we will get similar results to the FARE model. For the advantages of EFARE with respect to FARE, please have a look at the relevant section of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Validity: {sum(has_reached_recourse)/(len(has_reached_recourse))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-mcts",
   "language": "python",
   "name": "rl-mcts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
